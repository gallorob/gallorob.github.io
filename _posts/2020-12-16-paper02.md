---
theme: jekyll-theme-midnight
layout: default
title:  Paper notes_ Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

# Paper notes: Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients
#### Authors: [Joel Lehman](https://scholar.google.com/citations?user=GcvxHWQAAAAJ), [Jay Chen](https://scholar.google.com/citations?user=NPzHxnEAAAAJ), [Jeff Clune](https://scholar.google.com/citations?user=5TZ7f5wAAAAJ) and [Kenneth O. Stanley](https://scholar.google.com/citations?user=6Q6oO1MAAAAJ)
#### Full paper: [here](https://arxiv.org/abs/1712.06563)
#### Code: [here](https://github.com/uber-common/safemutations)

## tl;dr
Neuoroevolution works by perturbing the parameters vector of neural networks; this works well for small networks but it may easily break functionality for bigger, deeper networks. The paper introduces a family of operators that manage to avoid such problem by rescaling the perturbation vector applied such that the perturbed network's behaviour changes enough to continue exploration without breaking functionality.

## What's the problem with perturbations
Assume we have an agent that we want to evolve for a specific task. The agent itself is the parameters vector $\theta$ of some Neural Network. In NeuroEvolution, we add some noise to this vector, evaluate its performance via a fitness function and determine if it's a good fit for our task or not (spoiler alert: we use population search for a reason).

This is all well and good for small NNs, but as soon as we ramp up the complexity of the network, stuff breaks. More specifically, the network's behaviour after it gets perturbed is... well, *shit*. You may have a decent network, add some noise (even a small itsy-bitsy amount) and **BAM**, it suddenly behaves like a drunkard. Good like applying a search algorithm with this in mind.

Why's that, you ask? Well, simply put, each weight of the network has a different sensitivity to noise, and as the network gets larger it's much, much easier to have larger and larger repercussions on the end result.


## What's the proposed solution

What's the solution to this problem, then? Easy: we don't consider *just* the parameters space when we apply the perturbation, but also the outputs space!
Why? Because if we apply not just *any* noise, but an informed noise (with respect to the outputs, of course) we can get a perturbed network that behaves roughly the same as the non-perturbed one, yet differently enough to allow us to do some search!
How do we do this? Well, first of all we need to determine how a network behaves differently from another.
We do this by defining the divergence:
$$
\mathrm{Divergence}(\boldsymbol{\delta}; \boldsymbol{w}) = \frac{\sum_i \sum_k (\mathrm{NN}(\boldsymbol{X}_i; \boldsymbol{w})_k - \mathrm{NN}(\boldsymbol{X}_i; \boldsymbol{w} + \boldsymbol{\delta})_k)^2}{I}
$$
This is a MSE of the NN outputs before and after perturbations to the same inputs over $I$ experiences.
We can compute this by:
* Create the NN
* Collect an archive of inputs-outputs of the NN
* Modify the NN by adding the noise vector $\boldsymbol{\delta}$
* For each input in the archive, compute the modified NN's output and store it
* Compute the divergence

Easy enough, honestly. It requires as many forward passes as sampled experiences, though, so maybe keep your model and archives on [GPU](https://i.imgflip.com/45o1rl.jpg).

Ideally, now, we can specify how much divergence we want between the parent NN and the offspring NN and find a $\boldsymbol{\delta}$ that fits.

But how can we look for such a vector? Since it's a noise vector, we can't just search randomly, right? God no, it'd take forever. We have the power of **MATHS** on our side!

### SM through Rescaling

$$
\boldsymbol{\delta} = \boldsymbol{\delta}_{magnitude}\boldsymbol{\delta}_{direction}
$$

### SM through Gradients

$$
Y_{ik}(\boldsymbol{X}_i, \boldsymbol{\delta}; \boldsymbol{w}) = \mathrm{NN}(\boldsymbol{X}_i;\boldsymbol{w})_k + \boldsymbol{\delta}\nabla_\boldsymbol{w}\mathrm{NN}(\boldsymbol{X}_i;\boldsymbol{w})_k
$$

#### SM-G-ABS

$$
\boldsymbol{s} = \sqrt{\sum_k \left(\frac{\sum_i \mathrm{abs}(\nabla_{\boldsymbol{w}}\mathrm{NN}(\boldsymbol{X}_i)_k)^2}{\boldsymbol{I}}\right)}
$$

$$
\boldsymbol{\delta}_{adjusted} = \frac{\boldsymbol{\delta}}{\boldsymbol{s}}
$$

#### SM-G-SUM

$$
\boldsymbol{s}_{SUM} \approx \sqrt{\sum_k \left(\sum_i \nabla_{\boldsymbol{w}}\mathrm{NN}(\boldsymbol{X}_i)_k\right)^2}
$$

#### SM-G-SO

$$
\boldsymbol{\nabla}_\boldsymbol{w}(\mathrm{Divergence}(0 + \boldsymbol{\delta}_0; \boldsymbol{w})) \approx H_\boldsymbol{w}(\mathrm{Divergence}(0;\boldsymbol{w}))\boldsymbol{\delta}_0
$$


$$
\boldsymbol{s}_{SO} = \sqrt{\mathrm{abs}(H_\boldsymbol{w}(\mathrm{Divergence}(0;\boldsymbol{w}))\boldsymbol{\delta})}
$$

## Conclusions



[Paper notes](https://gallorob.github.io/paper-notes/)